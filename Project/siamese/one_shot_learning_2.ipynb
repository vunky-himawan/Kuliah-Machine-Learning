{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import InceptionResNetV2\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, optimizers\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense, Dropout, Lambda\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mtcnn import MTCNN\n",
    "import cv2\n",
    "import os\n",
    "from tensorflow.keras.preprocessing.image import img_to_array, array_to_img\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "from matplotlib import pyplot as plt\n",
    "from deepface import DeepFace\n",
    "import random\n",
    "import pickle\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.preprocessing import image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (160, 160, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class L2NormalizeLayer(layers.Layer):\n",
    "    def call(self, inputs):\n",
    "        return tf.math.l2_normalize(inputs, axis=1)\n",
    "\n",
    "class AbsoluteDifferenceLayer(layers.Layer):\n",
    "    def call(self, inputs):\n",
    "        return tf.abs(inputs[0] - inputs[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Base Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-19 02:42:15.654492: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:152] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n"
     ]
    }
   ],
   "source": [
    "base_model = InceptionResNetV2(\n",
    "    weights='imagenet',\n",
    "    include_top=False,\n",
    "    input_shape=input_shape\n",
    ")\n",
    "\n",
    "for layer in base_model.layers[-10:]: \n",
    "    layer.trainable = True\n",
    "\n",
    "inputs = layers.Input(shape=input_shape)\n",
    "x = base_model(inputs, training=False)\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dense(128, activation='relu')(x)\n",
    "x = L2NormalizeLayer()(x)\n",
    "\n",
    "model = models.Model(inputs, x, name=\"base_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Siamese Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_a = layers.Input(shape=input_shape)\n",
    "input_b = layers.Input(shape=input_shape)\n",
    "\n",
    "base_network = model\n",
    "\n",
    "embedding_a = base_network(input_a)\n",
    "embedding_b = base_network(input_b)\n",
    "\n",
    "distance = AbsoluteDifferenceLayer()([embedding_a, embedding_b])\n",
    "\n",
    "output = layers.Dense(1, activation='sigmoid')(distance)\n",
    "\n",
    "siamese_model = models.Model(inputs=[input_a, input_b], outputs=output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optimizers.Adam(learning_rate=1e-4)\n",
    "\n",
    "siamese_model.compile(\n",
    "    loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "    optimizer=optimizer,\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_2       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">160</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">160</span>,  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)                │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ input_layer_3       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">160</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">160</span>,  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)                │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ base_model          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │ <span style=\"color: #00af00; text-decoration-color: #00af00\">54,533,472</span> │ input_layer_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)        │                   │            │ input_layer_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ absolute_differenc… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ base_model[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>], │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">AbsoluteDifferenc…</span> │                   │            │ base_model[<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │        <span style=\"color: #00af00; text-decoration-color: #00af00\">129</span> │ absolute_differe… │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_2       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m160\u001b[0m, \u001b[38;5;34m160\u001b[0m,  │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │ \u001b[38;5;34m3\u001b[0m)                │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ input_layer_3       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m160\u001b[0m, \u001b[38;5;34m160\u001b[0m,  │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │ \u001b[38;5;34m3\u001b[0m)                │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ base_model          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │ \u001b[38;5;34m54,533,472\u001b[0m │ input_layer_2[\u001b[38;5;34m0\u001b[0m]… │\n",
       "│ (\u001b[38;5;33mFunctional\u001b[0m)        │                   │            │ input_layer_3[\u001b[38;5;34m0\u001b[0m]… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ absolute_differenc… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ base_model[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m], │\n",
       "│ (\u001b[38;5;33mAbsoluteDifferenc…\u001b[0m │                   │            │ base_model[\u001b[38;5;34m1\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │        \u001b[38;5;34m129\u001b[0m │ absolute_differe… │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">54,533,601</span> (208.03 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m54,533,601\u001b[0m (208.03 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">54,473,057</span> (207.80 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m54,473,057\u001b[0m (207.80 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">60,544</span> (236.50 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m60,544\u001b[0m (236.50 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "siamese_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Image From Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_images_from_directory(directory, target_size=(160, 160)):\n",
    "    '''Load all images and labels from a directory.'''\n",
    "    images = []\n",
    "    labels = []\n",
    "    label_map = {}\n",
    "\n",
    "    for label_idx, subdir in enumerate(os.listdir(directory)):\n",
    "        subdir_path = os.path.join(directory, subdir)\n",
    "        if os.path.isdir(subdir_path):\n",
    "            label_map[label_idx] = subdir\n",
    "            for file in os.listdir(subdir_path):\n",
    "                img_path = os.path.join(subdir_path, file)\n",
    "                img = image.load_img(img_path, target_size=target_size)\n",
    "                img_array = image.img_to_array(img) / 255.0\n",
    "                images.append(img_array)\n",
    "                labels.append(label_idx)\n",
    "    return np.array(images), np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_pairs(images, labels):\n",
    "    '''Create positive and negative pairs from images and labels.'''\n",
    "    pairs = []\n",
    "    targets = []\n",
    "\n",
    "    # Group images by labels\n",
    "    label_to_images = {}\n",
    "    for img, label in zip(images, labels):\n",
    "        if label not in label_to_images:\n",
    "            label_to_images[label] = []\n",
    "        label_to_images[label].append(img)\n",
    "\n",
    "    # Create positive and negative pairs\n",
    "    for label in label_to_images:\n",
    "        same_class_images = label_to_images[label]\n",
    "        for i in range(len(same_class_images)):\n",
    "            # Positive pair\n",
    "            img1 = same_class_images[i]\n",
    "            img2 = random.choice(same_class_images)\n",
    "            pairs.append([img1, img2])\n",
    "            targets.append(1)  # Positive label\n",
    "\n",
    "            # Negative pair\n",
    "            other_label = random.choice([l for l in label_to_images if l != label])\n",
    "            img3 = random.choice(label_to_images[other_label])\n",
    "            pairs.append([img1, img3])\n",
    "            targets.append(0)  # Negative label\n",
    "\n",
    "    return np.array(pairs), np.array(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training and validation data\n",
    "train_images, train_labels = load_images_from_directory('dataset/train')\n",
    "val_images, val_labels = load_images_from_directory('dataset/val')\n",
    "\n",
    "# Create pairs for training and validation\n",
    "x_train_pairs, y_train = create_pairs(train_images, train_labels)\n",
    "x_val_pairs, y_val = create_pairs(val_images, val_labels)\n",
    "\n",
    "# Split pairs into two inputs\n",
    "x_train_1, x_train_2 = x_train_pairs[:, 0], x_train_pairs[:, 1]\n",
    "x_val_1, x_val_2 = x_val_pairs[:, 0], x_val_pairs[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m553s\u001b[0m 5s/step - accuracy: 0.5212 - loss: 0.6897 - val_accuracy: 0.6694 - val_loss: 0.6437\n",
      "Epoch 2/20\n",
      "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m409s\u001b[0m 5s/step - accuracy: 0.7428 - loss: 0.6072 - val_accuracy: 0.7153 - val_loss: 0.6059\n",
      "Epoch 3/20\n",
      "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m405s\u001b[0m 5s/step - accuracy: 0.8013 - loss: 0.5679 - val_accuracy: 0.7514 - val_loss: 0.5871\n",
      "Epoch 4/20\n",
      "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m404s\u001b[0m 4s/step - accuracy: 0.8229 - loss: 0.5484 - val_accuracy: 0.7542 - val_loss: 0.5856\n",
      "Epoch 5/20\n",
      "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m437s\u001b[0m 5s/step - accuracy: 0.8622 - loss: 0.5295 - val_accuracy: 0.7792 - val_loss: 0.5745\n",
      "Epoch 6/20\n",
      "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m404s\u001b[0m 4s/step - accuracy: 0.8998 - loss: 0.5028 - val_accuracy: 0.7806 - val_loss: 0.5711\n",
      "Epoch 7/20\n",
      "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m408s\u001b[0m 5s/step - accuracy: 0.9344 - loss: 0.4870 - val_accuracy: 0.8000 - val_loss: 0.5657\n",
      "Epoch 8/20\n",
      "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m405s\u001b[0m 5s/step - accuracy: 0.9451 - loss: 0.4698 - val_accuracy: 0.7889 - val_loss: 0.5624\n",
      "Epoch 9/20\n",
      "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m406s\u001b[0m 5s/step - accuracy: 0.9514 - loss: 0.4549 - val_accuracy: 0.7833 - val_loss: 0.5592\n",
      "Epoch 10/20\n",
      "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m412s\u001b[0m 5s/step - accuracy: 0.9591 - loss: 0.4369 - val_accuracy: 0.8083 - val_loss: 0.5537\n",
      "Epoch 11/20\n",
      "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m408s\u001b[0m 5s/step - accuracy: 0.9700 - loss: 0.4165 - val_accuracy: 0.8153 - val_loss: 0.5499\n",
      "Epoch 12/20\n",
      "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m413s\u001b[0m 5s/step - accuracy: 0.9734 - loss: 0.4055 - val_accuracy: 0.7972 - val_loss: 0.5494\n",
      "Epoch 13/20\n",
      "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m404s\u001b[0m 4s/step - accuracy: 0.9772 - loss: 0.3865 - val_accuracy: 0.8097 - val_loss: 0.5469\n",
      "Epoch 14/20\n",
      "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m404s\u001b[0m 4s/step - accuracy: 0.9778 - loss: 0.3724 - val_accuracy: 0.7917 - val_loss: 0.5379\n",
      "Epoch 15/20\n",
      "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m416s\u001b[0m 5s/step - accuracy: 0.9774 - loss: 0.3657 - val_accuracy: 0.8111 - val_loss: 0.5376\n",
      "Epoch 16/20\n",
      "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m404s\u001b[0m 4s/step - accuracy: 0.9814 - loss: 0.3521 - val_accuracy: 0.8042 - val_loss: 0.5375\n",
      "Epoch 17/20\n",
      "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m404s\u001b[0m 4s/step - accuracy: 0.9782 - loss: 0.3419 - val_accuracy: 0.7889 - val_loss: 0.5331\n",
      "Epoch 18/20\n",
      "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m404s\u001b[0m 4s/step - accuracy: 0.9890 - loss: 0.3288 - val_accuracy: 0.8167 - val_loss: 0.5274\n",
      "Epoch 19/20\n",
      "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m408s\u001b[0m 5s/step - accuracy: 0.9869 - loss: 0.3198 - val_accuracy: 0.8014 - val_loss: 0.5206\n",
      "Epoch 20/20\n",
      "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4s/step - accuracy: 0.9895 - loss: 0.3095"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-19 05:00:45.971606: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 19661056 bytes after encountering the first element of size 19661056 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m416s\u001b[0m 5s/step - accuracy: 0.9895 - loss: 0.3096 - val_accuracy: 0.8097 - val_loss: 0.5183\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x7ee8583b23c0>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "siamese_model.fit(\n",
    "    [x_train_1, x_train_2], y_train,\n",
    "    validation_data=([x_val_1, x_val_2], y_val),\n",
    "    batch_size=32,\n",
    "    epochs=20\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 1s/step - accuracy: 0.8153 - loss: 0.5145\n",
      "Validation Loss: 0.5182710289955139, Validation Accuracy: 0.8097222447395325\n"
     ]
    }
   ],
   "source": [
    "loss, acc = siamese_model.evaluate([x_val_1, x_val_2], y_val)\n",
    "print(f\"Validation Loss: {loss}, Validation Accuracy: {acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "detector = MTCNN()\n",
    "\n",
    "def preprocess_image_with_mtcnn(img_path, target_size=(160, 160)):\n",
    "    '''Preprocess an image: detect face with MTCNN, crop, and resize.'''\n",
    "    # Load image\n",
    "    img = cv2.imread(img_path)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)  # Convert to RGB (MTCNN expects RGB format)\n",
    "    \n",
    "    # Detect faces\n",
    "    detections = detector.detect_faces(img)\n",
    "    \n",
    "    if detections:\n",
    "        # If a face is detected, crop the face region\n",
    "        x, y, width, height = detections[0]['box']  # Get bounding box of the first detected face\n",
    "        x, y = max(0, x), max(0, y)  # Ensure bounding box is within image bounds\n",
    "        cropped_face = img[y:y+height, x:x+width]\n",
    "    else:\n",
    "        # If no face is detected, use the full image\n",
    "        cropped_face = img\n",
    "    \n",
    "    # Resize cropped face or full image to target size\n",
    "    resized_img = cv2.resize(cropped_face, target_size)\n",
    "    \n",
    "    # Normalize pixel values and add batch dimension\n",
    "    img_array = resized_img / 255.0  # Normalize to [0, 1]\n",
    "    img_array = np.expand_dims(img_array, axis=0)\n",
    "    \n",
    "    return img_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 328ms/step\n",
      "Similarity Score: 0.4094489\n",
      "Hasil: Gambar kemungkinan wajah dari orang yang BERBEDA.\n"
     ]
    }
   ],
   "source": [
    "img_path_1 = 'dataset_copy/train/Achmad Raihan/IMG_20241117_161935.jpg'\n",
    "img_path_2 = 'jokowi_1.jpeg'\n",
    "\n",
    "img1 = preprocess_image_with_mtcnn(img_path_1)\n",
    "img2 = preprocess_image_with_mtcnn(img_path_2)\n",
    "\n",
    "# Prediksi dengan Siamese Network\n",
    "similarity_score = siamese_model.predict([img1, img2])\n",
    "\n",
    "print(\"Similarity Score:\", similarity_score[0][0])\n",
    "\n",
    "# Interpretasi Hasil\n",
    "if similarity_score[0][0] > 0.5:\n",
    "    print(\"Hasil: Gambar kemungkinan wajah dari orang yang SAMA.\")\n",
    "else:\n",
    "    print(\"Hasil: Gambar kemungkinan wajah dari orang yang BERBEDA.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "siamese_model.save('siamese_baru_lagi.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kuliah_ml",
   "language": "python",
   "name": "kuliah_ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
